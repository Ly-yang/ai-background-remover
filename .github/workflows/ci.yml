# .github/workflows/ci.yml - æŒç»­é›†æˆ
name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  # ä»£ç è´¨é‡æ£€æŸ¥
  code-quality:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install Python dependencies
      run: |
        pip install flake8 black isort mypy pytest
        pip install -r backend/requirements.txt
    
    - name: Lint with flake8
      run: |
        flake8 backend --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 backend --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    
    - name: Check code formatting with black
      run: black --check backend/
    
    - name: Check import sorting with isort
      run: isort --check-only backend/
    
    - name: Type checking with mypy
      run: mypy backend/ --ignore-missing-imports
    
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
    
    - name: Install frontend dependencies
      run: |
        cd frontend
        npm ci
    
    - name: Lint frontend code
      run: |
        cd frontend
        npm run lint
    
    - name: Check frontend formatting
      run: |
        cd frontend
        npm run format:check

  # åç«¯æµ‹è¯•
  backend-tests:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: testpass
          POSTGRES_DB: testdb
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        pip install -r backend/requirements.txt
        pip install pytest pytest-asyncio pytest-cov httpx
    
    - name: Run tests with coverage
      env:
        DATABASE_URL: postgresql://postgres:testpass@localhost:5432/testdb
        REDIS_URL: redis://localhost:6379
        SECRET_KEY: test-secret-key
      run: |
        cd backend
        pytest --cov=app --cov-report=xml --cov-report=html
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./backend/coverage.xml
        flags: backend
        name: backend-coverage

  # å‰ç«¯æµ‹è¯•
  frontend-tests:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
    
    - name: Install dependencies
      run: |
        cd frontend
        npm ci
    
    - name: Run tests
      run: |
        cd frontend
        npm test -- --coverage --watchAll=false
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./frontend/coverage/lcov.info
        flags: frontend
        name: frontend-coverage

  # å®‰å…¨æ‰«æ
  security-scan:
    if: github.event_name != 'pull_request' || github.event.pull_request.head.repo.full_name == github.repository
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'
    
    - name: Upload Trivy scan results to GitHub Security tab
      uses: github/codeql-action/upload-sarif@v3
      if: always()
      with:
        sarif_file: 'trivy-results.sarif'

  # Dockeræ„å»ºå’Œæ¨é€
  build-and-push:
    needs: [code-quality, backend-tests, frontend-tests]
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    permissions:
      contents: read
      packages: write
    
    strategy:
      matrix:
        service: [frontend, backend, ai-models]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/${{ matrix.service }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix={{branch}}-
          type=raw,value=latest,enable={{is_default_branch}}
    
    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: ./${{ matrix.service }}
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max
        platforms: linux/amd64,linux/arm64

  # éƒ¨ç½²åˆ°å¼€å‘ç¯å¢ƒ
  deploy-dev:
    needs: build-and-push
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/develop'
    environment: development
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Deploy to development
      uses: appleboy/ssh-action@v1.0.0
      with:
        host: ${{ secrets.DEV_HOST }}
        username: ${{ secrets.DEV_USERNAME }}
        key: ${{ secrets.DEV_SSH_KEY }}
        script: |
          cd /opt/ai-background-remover
          git pull origin develop
          docker-compose down
          docker-compose pull
          docker-compose up -d
          
  # éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
  deploy-prod:
    needs: build-and-push
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    environment: production
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Deploy to production
      uses: appleboy/ssh-action@v1.0.0
      with:
        host: ${{ secrets.PROD_HOST }}
        username: ${{ secrets.PROD_USERNAME }}
        key: ${{ secrets.PROD_SSH_KEY }}
        script: |
          cd /opt/ai-background-remover
          git pull origin main
          docker-compose -f infrastructure/docker/docker-compose.prod.yml down
          docker-compose -f infrastructure/docker/docker-compose.prod.yml pull
          docker-compose -f infrastructure/docker/docker-compose.prod.yml up -d
          
    - name: Health check
      run: |
        sleep 30
        curl -f https://your-domain.com/health || exit 1

---

# .github/workflows/security.yml - å®‰å…¨æ‰«æ
name: Security Scan

on:
  schedule:
    - cron: '0 2 * * 1'  # æ¯å‘¨ä¸€å‡Œæ™¨2ç‚¹
  workflow_dispatch:

jobs:
  dependency-scan:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    # Pythonä¾èµ–å®‰å…¨æ‰«æ
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install safety
      run: pip install safety
    
    - name: Check Python dependencies
      run: |
        pip install -r backend/requirements.txt
        safety check
    
    # Node.jsä¾èµ–å®‰å…¨æ‰«æ
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
    
    - name: Audit Node.js dependencies
      run: |
        cd frontend
        npm audit --audit-level high

  container-scan:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Build images for scanning
      run: |
        docker build -t ai-remover/backend:scan ./backend
        docker build -t ai-remover/frontend:scan ./frontend
        docker build -t ai-remover/ai-models:scan ./ai-models
    
    - name: Scan backend image
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: 'ai-remover/backend:scan'
        format: 'sarif'
        output: 'backend-scan.sarif'
    
    - name: Upload backend scan results
      uses: github/codeql-action/upload-sarif@v3
      with:
        sarif_file: 'backend-scan.sarif'
        category: 'backend-container'

---

# scripts/setup.sh - åˆå§‹åŒ–è„šæœ¬
#!/bin/bash

set -e

echo "ğŸš€ åˆå§‹åŒ– AI Background Remover é¡¹ç›®..."

# æ£€æŸ¥Dockerå’ŒDocker Compose
if ! command -v docker &> /dev/null; then
    echo "âŒ Docker æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…Docker"
    exit 1
fi

if ! command -v docker-compose &> /dev/null; then
    echo "âŒ Docker Compose æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…Docker Compose"
    exit 1
fi

# æ£€æŸ¥NVIDIA Dockeræ”¯æŒï¼ˆå¯é€‰ï¼‰
if command -v nvidia-docker &> /dev/null; then
    echo "âœ… NVIDIA Docker æ”¯æŒå·²å®‰è£…"
    GPU_SUPPORT=true
else
    echo "âš ï¸  NVIDIA Docker æ”¯æŒæœªå®‰è£…ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼"
    GPU_SUPPORT=false
fi

# åˆ›å»ºå¿…è¦çš„ç›®å½•
echo "ğŸ“ åˆ›å»ºé¡¹ç›®ç›®å½•..."
mkdir -p uploads/{original,processed,temp}
mkdir -p models/{rmbg,u2net,birefnet,sam,isnet}
mkdir -p logs
mkdir -p ssl

# ç”Ÿæˆç¯å¢ƒå˜é‡æ–‡ä»¶
echo "ğŸ”§ ç”Ÿæˆç¯å¢ƒå˜é‡æ–‡ä»¶..."
if [ ! -f .env ]; then
    cat > .env << EOF
# ç¯å¢ƒé…ç½®
ENV=development
DEBUG=true

# æ•°æ®åº“é…ç½®
POSTGRES_DB=airemover
POSTGRES_USER=postgres
POSTGRES_PASSWORD=$(openssl rand -base64 32)
DATABASE_URL=postgresql://postgres:$(openssl rand -base64 32)@db:5432/airemover

# Redisé…ç½®
REDIS_PASSWORD=$(openssl rand -base64 32)
REDIS_URL=redis://:$(openssl rand -base64 32)@redis:6379

# å®‰å…¨å¯†é’¥
SECRET_KEY=$(openssl rand -base64 64)
JWT_SECRET_KEY=$(openssl rand -base64 64)

# APIé…ç½®
API_V1_STR=/api/v1
ALLOWED_HOSTS=localhost,127.0.0.1,0.0.0.0

# æ–‡ä»¶ä¸Šä¼ é…ç½®
MAX_FILE_SIZE=10485760  # 10MB
UPLOAD_DIR=./uploads

# AIæ¨¡å‹é…ç½®
DEFAULT_MODEL=rmbg-2.0
MODEL_CACHE_SIZE=1000
GPU_ENABLED=$GPU_SUPPORT

# ç›‘æ§é…ç½®
GRAFANA_PASSWORD=$(openssl rand -base64 32)
GRAFANA_SECRET_KEY=$(openssl rand -base64 32)
EOF
    echo "âœ… ç¯å¢ƒå˜é‡æ–‡ä»¶å·²ç”Ÿæˆ"
else
    echo "âš ï¸  ç¯å¢ƒå˜é‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ç”Ÿæˆ"
fi

# è®¾ç½®æƒé™
echo "ğŸ”’ è®¾ç½®æ–‡ä»¶æƒé™..."
chmod +x scripts/*.sh
chmod 600 .env

# æ‹‰å–Dockeré•œåƒ
echo "ğŸ“¦ æ‹‰å–Dockeré•œåƒ..."
docker-compose pull || true

# æ„å»ºæœåŠ¡
echo "ğŸ—ï¸  æ„å»ºæœåŠ¡..."
docker-compose build

# åˆå§‹åŒ–æ•°æ®åº“
echo "ğŸ’¾ åˆå§‹åŒ–æ•°æ®åº“..."
docker-compose up -d db redis
sleep 10

# è¿è¡Œæ•°æ®åº“è¿ç§»
echo "ğŸ”„ è¿è¡Œæ•°æ®åº“è¿ç§»..."
docker-compose run --rm backend python -m alembic upgrade head

echo "âœ… é¡¹ç›®åˆå§‹åŒ–å®Œæˆï¼"
echo ""
echo "ğŸš€ å¯åŠ¨æœåŠ¡: docker-compose up -d"
echo "ğŸ“Š è®¿é—®åº”ç”¨: http://localhost:3000"
echo "ğŸ“‹ APIæ–‡æ¡£: http://localhost:8000/docs"
echo "ğŸ“ˆ ç›‘æ§é¢æ¿: http://localhost:3001 (admin/admin)"
echo ""

---

# scripts/deploy.sh - éƒ¨ç½²è„šæœ¬
#!/bin/bash

set -e

ENVIRONMENT=${1:-development}
VERSION=${2:-latest}

echo "ğŸš€ éƒ¨ç½² AI Background Remover åˆ° $ENVIRONMENT ç¯å¢ƒ..."

# æ£€æŸ¥å‚æ•°
if [[ "$ENVIRONMENT" != "development" && "$ENVIRONMENT" != "production" ]]; then
    echo "âŒ ç¯å¢ƒå‚æ•°é”™è¯¯ï¼Œè¯·ä½¿ç”¨ 'development' æˆ– 'production'"
    exit 1
fi

# è®¾ç½®éƒ¨ç½²é…ç½®
if [ "$ENVIRONMENT" = "production" ]; then
    COMPOSE_FILE="infrastructure/docker/docker-compose.prod.yml"
    ENV_FILE=".env.prod"
else
    COMPOSE_FILE="docker-compose.yml"
    ENV_FILE=".env"
fi

# æ£€æŸ¥ç¯å¢ƒæ–‡ä»¶
if [ ! -f "$ENV_FILE" ]; then
    echo "âŒ ç¯å¢ƒæ–‡ä»¶ $ENV_FILE ä¸å­˜åœ¨"
    exit 1
fi

# å¤‡ä»½å½“å‰ç‰ˆæœ¬
if [ "$ENVIRONMENT" = "production" ]; then
    echo "ğŸ’¾ å¤‡ä»½å½“å‰ç‰ˆæœ¬..."
    BACKUP_DIR="backups/$(date +%Y%m%d_%H%M%S)"
    mkdir -p "$BACKUP_DIR"
    
    # å¤‡ä»½æ•°æ®åº“
    docker-compose -f "$COMPOSE_FILE" exec -T db pg_dump -U postgres airemover > "$BACKUP_DIR/database.sql"
    
    # å¤‡ä»½ä¸Šä¼ æ–‡ä»¶
    cp -r uploads "$BACKUP_DIR/"
    
    echo "âœ… å¤‡ä»½å®Œæˆ: $BACKUP_DIR"
fi

# æ‹‰å–æœ€æ–°ä»£ç 
echo "ğŸ“¥ æ‹‰å–æœ€æ–°ä»£ç ..."
git pull origin main

# æ‹‰å–æœ€æ–°é•œåƒ
echo "ğŸ“¦ æ‹‰å–æœ€æ–°é•œåƒ..."
docker-compose -f "$COMPOSE_FILE" pull

# åœæ­¢æœåŠ¡
echo "â¹ï¸  åœæ­¢æ—§æœåŠ¡..."
docker-compose -f "$COMPOSE_FILE" down

# å¯åŠ¨æ–°æœåŠ¡
echo "â–¶ï¸  å¯åŠ¨æ–°æœåŠ¡..."
docker-compose -f "$COMPOSE_FILE" up -d

# ç­‰å¾…æœåŠ¡å¯åŠ¨
echo "â³ ç­‰å¾…æœåŠ¡å¯åŠ¨..."
sleep 30

# å¥åº·æ£€æŸ¥
echo "ğŸ” æ‰§è¡Œå¥åº·æ£€æŸ¥..."
for i in {1..10}; do
    if curl -f http://localhost/health > /dev/null 2>&1; then
        echo "âœ… æœåŠ¡å¯åŠ¨æˆåŠŸ"
        break
    fi
    
    if [ $i -eq 10 ]; then
        echo "âŒ æœåŠ¡å¯åŠ¨å¤±è´¥"
        
        # æ˜¾ç¤ºæ—¥å¿—
        echo "ğŸ“‹ æœåŠ¡æ—¥å¿—:"
        docker-compose -f "$COMPOSE_FILE" logs --tail=50
        
        exit 1
    fi
    
    echo "â³ ç­‰å¾…æœåŠ¡å¯åŠ¨... ($i/10)"
    sleep 10
done

# æ¸…ç†æ—§é•œåƒ
echo "ğŸ§¹ æ¸…ç†æ—§é•œåƒ..."
docker image prune -f

echo "âœ… éƒ¨ç½²å®Œæˆï¼"

if [ "$ENVIRONMENT" = "production" ]; then
    echo "ğŸŒ ç”Ÿäº§ç¯å¢ƒè®¿é—®åœ°å€: https://your-domain.com"
else
    echo "ğŸŒ å¼€å‘ç¯å¢ƒè®¿é—®åœ°å€: http://localhost:3000"
fi

---

# scripts/test.sh - æµ‹è¯•è„šæœ¬
#!/bin/bash

set -e

echo "ğŸ§ª è¿è¡Œæµ‹è¯•å¥—ä»¶..."

# å¯åŠ¨æµ‹è¯•ä¾èµ–æœåŠ¡
echo "ğŸš€ å¯åŠ¨æµ‹è¯•ä¾èµ–æœåŠ¡..."
docker-compose -f docker-compose.test.yml up -d db redis

# ç­‰å¾…æœåŠ¡å¯åŠ¨
sleep 10

# è¿è¡Œåç«¯æµ‹è¯•
echo "ğŸ è¿è¡Œåç«¯æµ‹è¯•..."
docker-compose -f docker-compose.test.yml run --rm backend python -m pytest tests/ -v --cov=app --cov-report=html --cov-report=xml

# è¿è¡Œå‰ç«¯æµ‹è¯•
echo "âš›ï¸  è¿è¡Œå‰ç«¯æµ‹è¯•..."
docker-compose -f docker-compose.test.yml run --rm frontend npm test -- --coverage --watchAll=false

# è¿è¡Œé›†æˆæµ‹è¯•
echo "ğŸ”— è¿è¡Œé›†æˆæµ‹è¯•..."
docker-compose -f docker-compose.test.yml run --rm integration-tests python -m pytest tests/integration/ -v

# è¿è¡Œæ€§èƒ½æµ‹è¯•
echo "âš¡ è¿è¡Œæ€§èƒ½æµ‹è¯•..."
docker-compose -f docker-compose.test.yml run --rm load-tests artillery run tests/load/basic-load.yml

# æ¸…ç†æµ‹è¯•ç¯å¢ƒ
echo "ğŸ§¹ æ¸…ç†æµ‹è¯•ç¯å¢ƒ..."
docker-compose -f docker-compose.test.yml down -v

# ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
echo "ğŸ“Š ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š..."
echo "âœ… æµ‹è¯•å®Œæˆï¼"
echo "ğŸ“‹ è¦†ç›–ç‡æŠ¥å‘Š: htmlcov/index.html"
echo "ğŸ“ˆ æ€§èƒ½æµ‹è¯•æŠ¥å‘Š: reports/load-test-report.html"

---

# scripts/backup.sh - å¤‡ä»½è„šæœ¬
#!/bin/bash

set -e

BACKUP_DIR="backups/$(date +%Y%m%d_%H%M%S)"
RETENTION_DAYS=30

echo "ğŸ’¾ å¼€å§‹å¤‡ä»½..."

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p "$BACKUP_DIR"

# å¤‡ä»½æ•°æ®åº“
echo "ğŸ“Š å¤‡ä»½æ•°æ®åº“..."
docker-compose exec -T db pg_dump -U postgres airemover | gzip > "$BACKUP_DIR/database.sql.gz"

# å¤‡ä»½Redisæ•°æ®
echo "ğŸ”´ å¤‡ä»½Redisæ•°æ®..."
docker-compose exec -T redis redis-cli BGSAVE
sleep 5
docker cp $(docker-compose ps -q redis):/data/dump.rdb "$BACKUP_DIR/redis.rdb"

# å¤‡ä»½ä¸Šä¼ æ–‡ä»¶
echo "ğŸ“ å¤‡ä»½ä¸Šä¼ æ–‡ä»¶..."
tar -czf "$BACKUP_DIR/uploads.tar.gz" uploads/

# å¤‡ä»½é…ç½®æ–‡ä»¶
echo "âš™ï¸  å¤‡ä»½é…ç½®æ–‡ä»¶..."
cp .env "$BACKUP_DIR/"
cp docker-compose*.yml "$BACKUP_DIR/"

# åˆ›å»ºå¤‡ä»½æ¸…å•
echo "ğŸ“‹ åˆ›å»ºå¤‡ä»½æ¸…å•..."
cat > "$BACKUP_DIR/backup_info.txt" << EOF
å¤‡ä»½æ—¶é—´: $(date)
å¤‡ä»½å†…å®¹:
- database.sql.gz: PostgreSQLæ•°æ®åº“å¤‡ä»½
- redis.rdb: Redisæ•°æ®å¤‡ä»½
- uploads.tar.gz: ç”¨æˆ·ä¸Šä¼ æ–‡ä»¶å¤‡ä»½
- .env: ç¯å¢ƒå˜é‡é…ç½®
- docker-compose*.yml: Docker Composeé…ç½®

æ¢å¤å‘½ä»¤:
1. æ¢å¤æ•°æ®åº“: gunzip -c database.sql.gz | docker-compose exec -T db psql -U postgres airemover
2. æ¢å¤Redis: docker cp redis.rdb \$(docker-compose ps -q redis):/data/dump.rdb && docker-compose restart redis
3. æ¢å¤ä¸Šä¼ æ–‡ä»¶: tar -xzf uploads.tar.gz
EOF

# è®¡ç®—å¤‡ä»½å¤§å°
BACKUP_SIZE=$(du -sh "$BACKUP_DIR" | cut -f1)
echo "âœ… å¤‡ä»½å®Œæˆ: $BACKUP_DIR ($BACKUP_SIZE)"

# æ¸…ç†æ—§å¤‡ä»½
echo "ğŸ§¹ æ¸…ç† $RETENTION_DAYS å¤©å‰çš„å¤‡ä»½..."
find backups/ -type d -name "????????_??????" -mtime +$RETENTION_DAYS -exec rm -rf {} \; 2>/dev/null || true

echo "ğŸ’¾ å¤‡ä»½ä»»åŠ¡å®Œæˆ"

---

# scripts/restore.sh - æ¢å¤è„šæœ¬
#!/bin/bash

set -e

BACKUP_PATH=$1

if [ -z "$BACKUP_PATH" ]; then
    echo "âŒ è¯·æŒ‡å®šå¤‡ä»½è·¯å¾„"
    echo "ä½¿ç”¨æ–¹æ³•: $0 <backup_path>"
    echo "ç¤ºä¾‹: $0 backups/20231201_120000"
    exit 1
fi

if [ ! -d "$BACKUP_PATH" ]; then
    echo "âŒ å¤‡ä»½è·¯å¾„ä¸å­˜åœ¨: $BACKUP_PATH"
    exit 1
fi

echo "ğŸ”„ ä»å¤‡ä»½æ¢å¤: $BACKUP_PATH"

# ç¡®è®¤æ“ä½œ
read -p "âš ï¸  è¿™å°†è¦†ç›–å½“å‰æ•°æ®ï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ(y/N): " -n 1 -r
echo
if [[ ! $REPLY =~ ^[Yy]$ ]]; then
    echo "âŒ æ“ä½œå·²å–æ¶ˆ"
    exit 1
fi

# åœæ­¢æœåŠ¡
echo "â¹ï¸  åœæ­¢æœåŠ¡..."
docker-compose down

# æ¢å¤æ•°æ®åº“
if [ -f "$BACKUP_PATH/database.sql.gz" ]; then
    echo "ğŸ“Š æ¢å¤æ•°æ®åº“..."
    docker-compose up -d db
    sleep 10
    
    # åˆ é™¤ç°æœ‰æ•°æ®åº“
    docker-compose exec db psql -U postgres -c "DROP DATABASE IF EXISTS airemover;"
    docker-compose exec db psql -U postgres -c "CREATE DATABASE airemover;"
    
    # æ¢å¤æ•°æ®
    gunzip -c "$BACKUP_PATH/database.sql.gz" | docker-compose exec -T db psql -U postgres airemover
    
    docker-compose down
fi

# æ¢å¤Redisæ•°æ®
if [ -f "$BACKUP_PATH/redis.rdb" ]; then
    echo "ğŸ”´ æ¢å¤Redisæ•°æ®..."
    docker-compose up -d redis
    sleep 5
    
    docker-compose stop redis
    docker cp "$BACKUP_PATH/redis.rdb" $(docker-compose ps -q redis):/data/dump.rdb
    docker-compose start redis
    
    docker-compose down
fi

# æ¢å¤ä¸Šä¼ æ–‡ä»¶
if [ -f "$BACKUP_PATH/uploads.tar.gz" ]; then
    echo "ğŸ“ æ¢å¤ä¸Šä¼ æ–‡ä»¶..."
    rm -rf uploads/
    tar -xzf "$BACKUP_PATH/uploads.tar.gz"
fi

# æ¢å¤é…ç½®æ–‡ä»¶
if [ -f "$BACKUP_PATH/.env" ]; then
    echo "âš™ï¸  æ¢å¤é…ç½®æ–‡ä»¶..."
    cp "$BACKUP_PATH/.env" .env
fi

# å¯åŠ¨æœåŠ¡
echo "â–¶ï¸  å¯åŠ¨æœåŠ¡..."
docker-compose up -d

# ç­‰å¾…æœåŠ¡å¯åŠ¨
sleep 30

# å¥åº·æ£€æŸ¥
echo "ğŸ” æ‰§è¡Œå¥åº·æ£€æŸ¥..."
if curl -f http://localhost/health > /dev/null 2>&1; then
    echo "âœ… æ¢å¤å®Œæˆï¼ŒæœåŠ¡æ­£å¸¸è¿è¡Œ"
else
    echo "âŒ æœåŠ¡å¯åŠ¨å¼‚å¸¸ï¼Œè¯·æ£€æŸ¥æ—¥å¿—"
    docker-compose logs --tail=20
    exit 1
fi

echo "ğŸ‰ æ•°æ®æ¢å¤å®Œæˆï¼"
